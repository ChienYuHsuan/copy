{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9430950-eafd-4b9a-8301-8d2e63abdb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import glob\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import pandas as pd\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc51b-64ff-4fe3-994a-2b4d8949fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "COLOR_DICT = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "                          Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "\n",
    "def adjustData(img,mask,flag_multi_class,num_class):\n",
    "    if(flag_multi_class):\n",
    "        img = img / 255\n",
    "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
    "        new_mask = np.zeros(mask.shape + (num_class,))\n",
    "        for i in range(num_class):\n",
    "            #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
    "            #index = np.where(mask == i)\n",
    "            #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
    "            #new_mask[index_mask] = 1\n",
    "            new_mask[mask == i,i] = 1\n",
    "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
    "        mask = new_mask\n",
    "    elif(np.max(img) > 1):\n",
    "        img = img / 255\n",
    "        mask = mask /255\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "\n",
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        yield (img,mask)\n",
    "\n",
    "\n",
    "\n",
    "def testGenerator(df,num_image,target_size = (256,256),flag_multi_class = False,as_gray = True):\n",
    "    for i in range(num_image):\n",
    "        img = io.imread(os.path.join(df['img_name'][i]),as_gray = as_gray)\n",
    "        img = img / 255\n",
    "        img = trans.resize(img,target_size)\n",
    "        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        img = np.reshape(img,(1,)+img.shape)\n",
    "        yield img\n",
    "\n",
    "\n",
    "def geneTrainNpy(image_path,mask_path,flag_multi_class = False,num_class = 2,image_prefix = \"image\",mask_prefix = \"mask\",image_as_gray = True,mask_as_gray = True):\n",
    "    image_name_arr = glob.glob(os.path.join(image_path,\"%s*.png\"%image_prefix))\n",
    "    image_arr = []\n",
    "    mask_arr = []\n",
    "    for index,item in enumerate(image_name_arr):\n",
    "        img = io.imread(item,as_gray = image_as_gray)\n",
    "        img = np.reshape(img,img.shape + (1,)) if image_as_gray else img\n",
    "        mask = io.imread(item.replace(image_path,mask_path).replace(image_prefix,mask_prefix),as_gray = mask_as_gray)\n",
    "        mask = np.reshape(mask,mask.shape + (1,)) if mask_as_gray else mask\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "        image_arr.append(img)\n",
    "        mask_arr.append(mask)\n",
    "    image_arr = np.array(image_arr)\n",
    "    mask_arr = np.array(mask_arr)\n",
    "    return image_arr,mask_arr\n",
    "\n",
    "\n",
    "def labelVisualize(num_class,color_dict,img):\n",
    "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i,:] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "\n",
    "\n",
    "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        img = img.astype(np.uint8)\n",
    "        io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c1acbd-179b-4857-abe5-65264f0c2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (256,256,3)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d0a47d-880a-489a-9971-551c0d738e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5470 images belonging to 1 classes.\n",
      "Found 5480 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_11/conv2d_267/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2238605/1221831835.py\", line 18, in <module>\n      model.fit(myGene,steps_per_epoch=100,epochs=1,callbacks=[model_checkpoint])\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py\", line 275, in call\n      return self.activation(outputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 311, in relu\n      return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4956, in relu\n      x = tf.nn.relu(x)\nNode: 'model_11/conv2d_267/Relu'\ninput depth must be evenly divisible by filter depth:  vs \n\t [[{{node model_11/conv2d_267/Relu}}]] [Op:__inference_train_function_35823]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m unet()\n\u001b[1;32m     17\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet_membrane.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyGene\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./BFC/origin/*\u001b[39m\u001b[38;5;124m'\u001b[39m),columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_11/conv2d_267/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2238605/1221831835.py\", line 18, in <module>\n      model.fit(myGene,steps_per_epoch=100,epochs=1,callbacks=[model_checkpoint])\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py\", line 275, in call\n      return self.activation(outputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 311, in relu\n      return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4956, in relu\n      x = tf.nn.relu(x)\nNode: 'model_11/conv2d_267/Relu'\ninput depth must be evenly divisible by filter depth:  vs \n\t [[{{node model_11/conv2d_267/Relu}}]] [Op:__inference_train_function_35823]"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(2,'BFC','origin','all_0515',data_gen_args,save_to_dir = None)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "model.fit(myGene,steps_per_epoch=100,epochs=1,callbacks=[model_checkpoint])\n",
    "\n",
    "df = pd.DataFrame(glob.glob(r'./BFC/origin/*'),columns=['img_name'])\n",
    "df['img_name'] = df['img_name'].apply(lambda x : x.replace(\"\\\\\",\"/\"))\n",
    "\n",
    "testGene = testGenerator(df,len(df))\n",
    "results = model.predict_generator(testGene,len(df),verbose=1)\n",
    "saveResult(\"BFC/unet_segmetation\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e7f7b-0c4c-49f8-9510-1c05533de9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f06866f-d57e-4021-b07c-0ca56b529ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     82\u001b[0m image_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[1;32m     86\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m num_samples)\n",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m, in \u001b[0;36mgenerate_synthetic_data\u001b[0;34m(num_samples, image_shape)\u001b[0m\n\u001b[1;32m     14\u001b[0m center_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m30\u001b[39m, image_shape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     15\u001b[0m radius \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m y, x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mogrid\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m mask \u001b[38;5;241m=\u001b[39m ((x \u001b[38;5;241m-\u001b[39m center_x)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (y \u001b[38;5;241m-\u001b[39m center_y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m radius\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     19\u001b[0m masks[i][mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/index_tricks.py:188\u001b[0m, in \u001b[0;36mnd_grid.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse:\n\u001b[1;32m    187\u001b[0m     slobj \u001b[38;5;241m=\u001b[39m [_nx\u001b[38;5;241m.\u001b[39mnewaxis]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(size)\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    189\u001b[0m         slobj[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    190\u001b[0m         nn[k] \u001b[38;5;241m=\u001b[39m nn[k][\u001b[38;5;28mtuple\u001b[39m(slobj)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "\n",
    "# Generate synthetic cell images and segmentation masks\n",
    "def generate_synthetic_data(num_samples, image_shape):\n",
    "    images = np.random.randint(0, 256, size=(num_samples,) + image_shape)\n",
    "    masks = np.zeros((num_samples,) + image_shape[:2]+(1,))\n",
    "    \n",
    "    # Add circular cell objects to the images and create corresponding masks\n",
    "    for i in range(num_samples):\n",
    "        center_x = np.random.randint(30, image_shape[0]-30)\n",
    "        center_y = np.random.randint(30, image_shape[1]-30)\n",
    "        radius = np.random.randint(5, 20)\n",
    "        \n",
    "        y, x = np.ogrid[:image_shape[0], :image_shape[1]]\n",
    "        mask = ((x - center_x)**2 + (y - center_y)**2) <= radius**2\n",
    "        masks[i][mask] = 1\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "# Define U-Net architecture for cell segmentation\n",
    "def unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Contracting Path\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    # ... Add more convolutional blocks (downsampling) ...\n",
    "\n",
    "    # Expanding Path\n",
    "    # ... Add more convolutional blocks (upsampling) ...\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv10)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Generate synthetic cell images and masks\n",
    "num_samples = 1000\n",
    "image_shape = (256, 256, 3)\n",
    "images, masks = generate_synthetic_data(num_samples, image_shape)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "split = int(0.8 * num_samples)\n",
    "train_images, train_masks = images[:split], masks[:split]\n",
    "test_images, test_masks = images[split:], masks[split:]\n",
    "\n",
    "# Preprocess the input data\n",
    "train_images = train_images / 255.0\n",
    "train_masks = train_masks.astype('float32')\n",
    "\n",
    "test_images = test_images / 255.0\n",
    "test_masks = test_masks.astype('float32')\n",
    "\n",
    "# Instantiate and compile the U-Net model\n",
    "model = unet(input_shape=image_shape)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_masks, batch_size=16, epochs=2, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_masks)\n",
    "\n",
    "# Perform cell segmentation on a test image\n",
    "test_image = test_images[0]\n",
    "predicted_mask = model.predict(np.expand_dims(test_image, axis=0))[0]\n",
    "\n",
    "# Visualize the results\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_image)\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(test_masks[0], cmap='gray')\n",
    "plt.title('Ground Truth Mask')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted_mask, cmap='gray')\n",
    "plt.title('Predicted Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11886ed-e6d9-4700-8cca-c660b1ac8d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae930c-a1d4-4e29-911d-73bd1c9c07e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7411692-6167-4476-901a-cca4935a15d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ee8156-601c-4909-9c06-5ac18db8ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Cropping2D, Concatenate\n",
    "\n",
    "class UnetUtils():\n",
    "    \n",
    "    \"\"\" \n",
    "    Unet Model design utillities framework.\n",
    "    \n",
    "    This module provides a convenient way to create different layers/blocks\n",
    "    which the UNet network is based upon. It consists of a contracting\n",
    "    path and an expansive path. Both these paths are joined by a bottleneck block.\n",
    "    \n",
    "    The different blocks involved in the design of the network can be referenced @ \n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    \n",
    "    Source:\n",
    "        https://arxiv.org/pdf/1505.04597\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def contracting_block(self, input_layer, filters, padding, kernel_size = 3):\n",
    "        \n",
    "        \"\"\" \n",
    "        UNet Contracting block\n",
    "        Perform two unpadded convolutions with a specified number of filters and downsample\n",
    "        through max-pooling.\n",
    "        \n",
    "        Args:\n",
    "            input_layer: the input layer on which the current layers should work upon.\n",
    "            filters (int): Number of filters in convolution.\n",
    "            kernel_size (int/tuple): Index of block. Default is 3.\n",
    "            padding (\"valid\" or \"same\"): Default is \"valid\" (no padding involved).\n",
    "            \n",
    "        Return:\n",
    "            Tuple of convolved ``inputs`` after and before downsampling\n",
    "        \"\"\"\n",
    "        \n",
    "        # two 3x3 convolutions (unpadded convolutions), each followed by\n",
    "        # a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2\n",
    "        # for downsampling.\n",
    "        conv = Conv2D(filters = filters, \n",
    "                      kernel_size = kernel_size, \n",
    "                      activation = tf.nn.relu, \n",
    "                      padding = padding)(input_layer)\n",
    "\n",
    "        conv = Conv2D(filters = filters, \n",
    "                      kernel_size = kernel_size, \n",
    "                      activation = tf.nn.relu, \n",
    "                      padding = padding)(conv)\n",
    "\n",
    "        pool = MaxPooling2D(pool_size = 2, \n",
    "                            strides = 2)(conv)\n",
    "\n",
    "        return conv, pool\n",
    "\n",
    "    def bottleneck_block(self, input_layer, filters, padding, kernel_size = 3, strides = 1):\n",
    "        \n",
    "        \"\"\" \n",
    "        UNet bottleneck block\n",
    "        \n",
    "        Performs 2 unpadded convolutions with a specified number of filters.\n",
    "        \n",
    "        Args:\n",
    "            input_layer: the input layer on which the current layers should work upon.\n",
    "            filters (int): Number of filters in convolution.\n",
    "            kernel_size (int/tuple): Index of block. Default is 3.\n",
    "            padding (\"valid\" or \"same\"): Default is \"valid\" (no padding involved).\n",
    "            strides: An integer or tuple/list of 2 integers, specifying the strides \n",
    "                     of the convolution along the height and width. Default is 1.\n",
    "        Return:\n",
    "            The convolved ``inputs``.\n",
    "        \"\"\"\n",
    "        \n",
    "        # two 3x3 convolutions (unpadded convolutions), each followed by\n",
    "        # a rectified linear unit (ReLU)\n",
    "        conv = Conv2D(filters = filters, \n",
    "                      kernel_size = kernel_size, \n",
    "                      padding = padding,\n",
    "                      strides = strides, \n",
    "                      activation = tf.nn.relu)(input_layer)\n",
    "\n",
    "        conv = Conv2D(filters = filters, \n",
    "                      kernel_size = kernel_size, \n",
    "                      padding = padding,\n",
    "                      strides = strides, \n",
    "                      activation = tf.nn.relu)(conv)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def expansive_block(self, input_layer, skip_conn_layer, filters, padding, kernel_size = 3, strides = 1):\n",
    "        \n",
    "        \"\"\" \n",
    "        UNet expansive (upsample) block.\n",
    "        \n",
    "        Transpose convolution which doubles the spatial dimensions (height and width) \n",
    "        of the incoming feature maps and creates the skip connections with the corresponding \n",
    "        feature maps from the contracting (downsample) path. These skip connections bring the feature maps \n",
    "        from earlier layers helping the network to generate better semantic feature maps.\n",
    "        \n",
    "        Perform two unpadded convolutions with a specified number of filters \n",
    "        and upsamples the incomming feature map.\n",
    "        \n",
    "        Args:\n",
    "            input_layer: the input layer on which the current layers should work upon.\n",
    "            skip_connection: The feature map from the contracting (downsample) path from which the \n",
    "                             skip connection has to be created.\n",
    "            filters (int): Number of filters in convolution.\n",
    "            kernel_size (int/tuple): Index of block. Default is 3.\n",
    "            padding (\"valid\" or \"same\"): Default is \"valid\" (no padding involved).\n",
    "            strides: An integer or tuple/list of 2 integers, specifying the strides \n",
    "                     of the convolution along the height and width. Default is 1.\n",
    "                     \n",
    "        Return:\n",
    "            The upsampled feature map.\n",
    "        \"\"\"\n",
    "        \n",
    "        # up sample the feature map using transpose convolution operations.\n",
    "        transConv = Conv2DTranspose(filters = filters, \n",
    "                                    kernel_size = (2, 2),\n",
    "                                    strides = 2, \n",
    "                                    padding = padding)(input_layer)\n",
    "        \n",
    "        # crop the source feature map so that the skip connection can be established.\n",
    "        # the original paper implemented unpadded convolutions. So cropping is necessary \n",
    "        # due to the loss of border pixels in every convolution.\n",
    "        # establish the skip connections.\n",
    "        if padding == \"valid\":\n",
    "            cropped = self.crop_tensor(skip_conn_layer, transConv)\n",
    "            concat = Concatenate()([transConv, cropped])\n",
    "        else:\n",
    "            concat = Concatenate()([transConv, skip_conn_layer])\n",
    "        \n",
    "        # two 3x3 convolutions, each followed by a ReLU\n",
    "        up_conv = Conv2D(filters = filters, \n",
    "                         kernel_size = kernel_size, \n",
    "                         padding = padding, \n",
    "                         activation = tf.nn.relu)(concat)\n",
    "\n",
    "        up_conv = Conv2D(filters = filters, \n",
    "                         kernel_size = kernel_size, \n",
    "                         padding = padding, \n",
    "                         activation = tf.nn.relu)(up_conv)\n",
    "\n",
    "        return up_conv\n",
    "    \n",
    "    def crop_tensor(self, source_tensor, target_tensor):\n",
    "        \n",
    "        \"\"\"\n",
    "        Center crops the source tensor to the size of the target tensor size.\n",
    "        The tensor shape format is [batchsize, height, width, channels]\n",
    "        \n",
    "        Args:\n",
    "            source_tensor: the tensor that is to be cropped.\n",
    "            target_tensor: the tensor to whose size the \n",
    "                           source needs to be cropped to.\n",
    "                           \n",
    "        Return:\n",
    "            the cropped version of the source tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        target_tensor_size = target_tensor.shape[2]\n",
    "        source_tensor_size = source_tensor.shape[2]\n",
    "        \n",
    "        # calculate the delta to ensure correct cropping.\n",
    "        delta = source_tensor_size - target_tensor_size\n",
    "        delta = delta // 2\n",
    "        \n",
    "        cropped_source = source_tensor[:, delta:source_tensor_size - delta, delta:source_tensor_size - delta, :]\n",
    "        \n",
    "        return cropped_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653f7f9a-172f-4544-8cf0-d74073b19280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "\n",
    "UnetUtils = UnetUtils()\n",
    "\n",
    "class Unet():\n",
    "    \n",
    "    \"\"\" \n",
    "    Unet Model design.\n",
    "    \n",
    "    This module consumes the Unet utilities framework moule and designs the Unet network.\n",
    "    It consists of a contracting path and an expansive path. Both these paths are joined \n",
    "    by a bottleneck block.\n",
    "    \n",
    "    The different blocks involved in the design of the network can be referenced @ \n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    \n",
    "    Source:\n",
    "        https://arxiv.org/pdf/1505.04597\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape = (572, 572, 1), filters = [64, 128, 256, 512, 1024], padding = \"valid\"):\n",
    "        \"\"\"\n",
    "        \n",
    "        Initialize the Unet framework and the model parameters - input_shape, \n",
    "        filters and padding type. \n",
    "        \n",
    "        Args:\n",
    "            input_shape: The shape of the input to the network. A tuple comprising of (img_height, img_width, channels).\n",
    "                         Original paper implementation is (572, 572, 1).\n",
    "            filters: a collection of filters denoting the number of components to be used at each blocks along the \n",
    "                     contracting and expansive paths. The original paper implementation for number of filters along the \n",
    "                     contracting and expansive paths are [64, 128, 256, 512, 1024].\n",
    "            padding: the padding type to be used during the convolution step. The original paper used unpadded convolutions \n",
    "                     which is of type \"valid\".\n",
    "         \n",
    "        **Remarks: The default values are as per the implementation in the original paper @ https://arxiv.org/pdf/1505.04597\n",
    "        \n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = filters\n",
    "        self.padding = padding\n",
    "    \n",
    "    def Build_UNetwork(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds the Unet Model network.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "         \n",
    "        Return:\n",
    "            The Unet Model.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        UnetInput = Input(self.input_shape)\n",
    "        \n",
    "        # the contracting path. \n",
    "        # the last item in the filetrs collection points to the number of filters in the bottleneck block.\n",
    "        conv1, pool1 = UnetUtils.contracting_block(input_layer = UnetInput, filters = self.filters[0], padding = self.padding)\n",
    "        conv2, pool2 = UnetUtils.contracting_block(input_layer = pool1, filters = self.filters[1], padding = self.padding)\n",
    "        conv3, pool3 = UnetUtils.contracting_block(input_layer = pool2, filters = self.filters[2], padding = self.padding)\n",
    "        conv4, pool4 = UnetUtils.contracting_block(input_layer = pool3, filters = self.filters[3], padding = self.padding)\n",
    "        \n",
    "        # bottleneck block connecting the contracting and the expansive paths.\n",
    "        bottleNeck = UnetUtils.bottleneck_block(pool4, filters = self.filters[4], padding = self.padding)\n",
    "\n",
    "        # the expansive path.\n",
    "        upConv1 = UnetUtils.expansive_block(bottleNeck, conv4, filters = self.filters[3], padding = self.padding) \n",
    "        upConv2 = UnetUtils.expansive_block(upConv1, conv3, filters = self.filters[2], padding = self.padding) \n",
    "        upConv3 = UnetUtils.expansive_block(upConv2, conv2, filters = self.filters[1], padding = self.padding) \n",
    "        upConv4 = UnetUtils.expansive_block(upConv3, conv1, filters = self.filters[0], padding = self.padding) \n",
    "\n",
    "        UnetOutput = Conv2D(1, (1, 1), padding = self.padding, activation = tf.math.sigmoid)(upConv4)\n",
    "        \n",
    "        model = Model(UnetInput, UnetOutput, name = \"UNet\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def CompileAndSummarizeModel(self, model, optimizer = \"adam\", loss = \"binary_crossentropy\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compiles and displays the model summary of the Unet model.\n",
    "        \n",
    "        Args:\n",
    "            model: The Unet model.\n",
    "            optimizer: model optimizer. Default is the adam optimizer.\n",
    "            loss: the loss function. Default is the binary cross entropy loss.\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "        model.compile(optimizer = optimizer, loss = loss, metrics = [\"acc\"])\n",
    "        model.summary()\n",
    "        \n",
    "    def plotModel(self, model, to_file = 'unet.png', show_shapes = True, dpi = 96):\n",
    "        \n",
    "        \"\"\"\n",
    "        Saves the Unet model to a file.\n",
    "        \n",
    "        Args:\n",
    "            model: the Unet model. \n",
    "            to_file: the file name to save the model. Default name - 'unet.png'.\n",
    "            show_shapes: whether to display shape information. Default = True.\n",
    "            dpi: dots per inch. Default value is 96.\n",
    "            \n",
    "        Return:\n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tf.keras.utils.plot_model(model, to_file = to_file, show_shapes = show_shapes, dpi = dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943d2c00-2bce-4aa1-a8f8-22731ab93aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d8e51c-7f47-45b7-8d9f-161927cbe282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:57:09.432873: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = unet.Build_UNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eed7cd0-2fa5-4194-a2e0-7639a170a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 572, 572, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 570, 570, 64  640         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 568, 568, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 284, 284, 64  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 282, 282, 12  73856       ['max_pooling2d[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 280, 280, 12  147584      ['conv2d_2[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 140, 140, 12  0          ['conv2d_3[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 138, 138, 25  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 136, 136, 25  590080      ['conv2d_4[0][0]']               \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 68, 68, 256)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 66, 66, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0          ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 30, 30, 1024  4719616     ['max_pooling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 28, 28, 1024  9438208     ['conv2d_8[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 56, 56, 512)  2097664    ['conv2d_9[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 56, 56, 512)  0          ['conv2d_7[0][0]']               \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 56, 56, 1024  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 54, 54, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 52, 52, 512)  2359808     ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 104, 104, 25  524544     ['conv2d_11[0][0]']              \n",
      " spose)                         6)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 104, 104, 25  0          ['conv2d_5[0][0]']               \n",
      " icingOpLambda)                 6)                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 104, 104, 51  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                2)                                'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 102, 102, 25  1179904     ['concatenate_1[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 100, 100, 25  590080      ['conv2d_12[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 200, 200, 12  131200     ['conv2d_13[0][0]']              \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 200, 200, 12  0          ['conv2d_3[0][0]']               \n",
      " icingOpLambda)                 8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 200, 200, 25  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                6)                                'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 198, 198, 12  295040      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 196, 196, 12  147584      ['conv2d_14[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 392, 392, 64  32832      ['conv2d_15[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 392, 392, 64  0          ['conv2d_1[0][0]']               \n",
      " icingOpLambda)                 )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 392, 392, 12  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                8)                                'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 390, 390, 64  73792       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 388, 388, 64  36928       ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 388, 388, 1)  65          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    unet.CompileAndSummarizeModel(model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c245aee-4484-4b53-b221-0b34b0568fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class NucleiDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    \"\"\"\n",
    "    The custom data generator class generates and feeds data to\n",
    "    the model dynamically in batches during the training phase.\n",
    "    \n",
    "    This generator generates batched of data for the dataset available @\n",
    "    Find the nuclei in divergent images to advance medical discovery -\n",
    "    https://www.kaggle.com/c/data-science-bowl-2018\n",
    "    \n",
    "    **\n",
    "    tf.keras.utils.Sequence is the root class for \n",
    "    Custom Data Generators.\n",
    "    **\n",
    "    \n",
    "    Args:\n",
    "        image_ids: the ids of the image.\n",
    "        img_path: the full path of the image directory.\n",
    "        batch_size: no. of images to be included in a batch feed. Default is set to 8.\n",
    "        image_size: size of the image. Default is set to 128 as per the data available.\n",
    "        \n",
    "    Ref: https://dzlab.github.io/dltips/en/keras/data-generator/\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, image_ids, img_path, batch_size = 8, image_size = 128):\n",
    "        \n",
    "        self.ids = image_ids\n",
    "        self.path = img_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, item):\n",
    "        \n",
    "        \"\"\"\n",
    "        loads the specified image.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # the name for parent of parent directory where the image is located and the name of the image are same.\n",
    "        # an example directory breakup is shown below -\n",
    "        # - data-science-bowl-2018/\n",
    "        #      - stage1_train/\n",
    "        #          - abc\n",
    "        #             - image\n",
    "        #                  - abc\n",
    "        #             - mask\n",
    "        full_image_path = os.path.join(self.path, item, \"images\", item) + \".png\"\n",
    "        mask_dir_path = os.path.join(self.path, item, \"masks/\")\n",
    "        all_masks = os.listdir(mask_dir_path)\n",
    "        \n",
    "        # load the images\n",
    "        image = cv2.imread(full_image_path, 1)\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        \n",
    "        masked_img = np.zeros((self.image_size, self.image_size, 1))\n",
    "        \n",
    "        # load and prepare the corresponding mask.\n",
    "        for mask in all_masks:\n",
    "            fullPath = mask_dir_path + mask\n",
    "            _masked_img = cv2.imread(fullPath, -1)\n",
    "            _masked_img = cv2.resize(_masked_img, (self.image_size, self.image_size))\n",
    "            _masked_img = np.expand_dims(_masked_img, axis = -1)\n",
    "            masked_img = np.maximum(masked_img, _masked_img)\n",
    "            \n",
    "        # mormalize the mask and the image. \n",
    "        image = image/255.0\n",
    "        masked_img = masked_img/255.0\n",
    "        \n",
    "        return image, masked_img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns a single batch of data.\n",
    "        \n",
    "        Args:\n",
    "            index: the batch index.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # edge case scenario where there are still some items left\n",
    "        # after segregatings the images into batches of size batch_size.\n",
    "        # the items left out will form one batch at the end.\n",
    "        if(index + 1) * self.batch_size > len(self.ids):\n",
    "            self.batch_size = len(self.ids) - index * self.batch_size\n",
    "        \n",
    "        # group the items into a batch.\n",
    "        batch = self.ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        \n",
    "        image = []\n",
    "        mask  = []\n",
    "        \n",
    "        # load the items in the current batch\n",
    "        for item in batch:\n",
    "            img, masked_img = self.__load__(item)\n",
    "            image.append(img)\n",
    "            mask.append(masked_img)\n",
    "        \n",
    "        image = np.array(image)\n",
    "        mask  = np.array(mask)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        optional method to run some logic at the end of each epoch: e.g. reshuffling\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the number of batches\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.ids)/float(self.batch_size)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a72c50dd-666b-4873-8ebc-cbba76a893be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7754e86-dd10-47fc-953c-f4d62e2230d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "image_channels = 3\n",
    "image_dir = \"./BFC/\"\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "# there are a total of 670 items at the train_path directory.\n",
    "# so fixing 600 of data available for training set\n",
    "# 50 for validation set and 20 for test set.\n",
    "validation_data_size = 50\n",
    "test_data_size = 20\n",
    "train_data_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41614b49-f962-4641-84e3-ed45cc299d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeImageAndMask(image, mask, prediction_img = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Displays the image, mask and the predicted mask\n",
    "    of the input image.\n",
    "    \n",
    "    Args:\n",
    "        image: the original image.\n",
    "        mask: the given mask of the image.\n",
    "        prediction_img: the predicted mask of the image.\n",
    "        \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(hspace = 0.6, wspace = 0.6)\n",
    "    fig.suptitle('Image & Mask(s)', fontsize = 15)\n",
    "    fig.subplots_adjust(top = 1.15)\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    ax.imshow(image)\n",
    "    setTitleAndRemoveTicks(ax, 'Microscopic\\nImage')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    ax.imshow(np.reshape(mask, (image_size, image_size)), cmap = \"gray\")\n",
    "    setTitleAndRemoveTicks(ax, 'Original\\nMask')\n",
    "    \n",
    "    if prediction_img is not None:\n",
    "        ax = fig.add_subplot(1, 3, 3)\n",
    "        ax.imshow(np.reshape(prediction_img, (image_size, image_size)), cmap = \"gray\")\n",
    "        setTitleAndRemoveTicks(ax, 'Predicted\\nMask')\n",
    "    \n",
    "def setTitleAndRemoveTicks(axes, title):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sets the sub-plot title and removes the \n",
    "    x & y ticks on the respective axes.\n",
    "    \n",
    "    Args:\n",
    "        axes: the subplot.\n",
    "        title: title of the subplot.\n",
    "        \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # set plot title\n",
    "    axes.title.set_text(title)\n",
    "    \n",
    "    # remove the ticks\n",
    "    axes.set_xticks([])\n",
    "    axes.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22308644-0b0b-4fbc-9872-3b48b22568e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = next(os.walk(image_dir))[1]\n",
    "\n",
    "# partition the data into train, test and validation sets.\n",
    "testing_data_ids = image_ids[:test_data_size]\n",
    "validation_data_ids = image_ids[:validation_data_size]\n",
    "training_data_ids = image_ids[:train_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfea052e-6fb1-4951-a501-2f2e1745954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['origin', 'all_0515', '.ipynb_checkpoints', 'unet_segmetation', 'single_0515']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "945a5f7d-e99e-4ed1-936c-029e9f52040a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './BFC/origin/masks/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m temp_data_generator \u001b[38;5;241m=\u001b[39m NucleiDataGenerator(image_ids \u001b[38;5;241m=\u001b[39m training_data_ids, \n\u001b[1;32m      2\u001b[0m                                           img_path \u001b[38;5;241m=\u001b[39m image_dir, \n\u001b[1;32m      3\u001b[0m                                           batch_size \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m      4\u001b[0m                                           image_size \u001b[38;5;241m=\u001b[39m image_size)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# get one batch of data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Dimension Details:\u001b[39m\u001b[38;5;124m\"\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape, masks\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[7], line 104\u001b[0m, in \u001b[0;36mNucleiDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# load the items in the current batch\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m--> 104\u001b[0m     img, masked_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     image\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m    106\u001b[0m     mask\u001b[38;5;241m.\u001b[39mappend(masked_img)\n",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m, in \u001b[0;36mNucleiDataGenerator.__load__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     56\u001b[0m full_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, item, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, item) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m mask_dir_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, item, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m all_masks \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_dir_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# load the images\u001b[39;00m\n\u001b[1;32m     61\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(full_image_path, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './BFC/origin/masks/'"
     ]
    }
   ],
   "source": [
    "temp_data_generator = NucleiDataGenerator(image_ids = training_data_ids, \n",
    "                                          img_path = image_dir, \n",
    "                                          batch_size = batch_size, \n",
    "                                          image_size = image_size)\n",
    "\n",
    "# get one batch of data\n",
    "images, masks = temp_data_generator.__getitem__(0)\n",
    "print(\"Batch Dimension Details:\", images.shape, masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd01685-090f-455c-9f7e-8d6d0ac47603",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpath\u001b[49m, item, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, item) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;66;03m# mask_dir_path = os.path.join(self.path, item, \"masks/\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# all_masks = os.listdir(mask_dir_path)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "full_image_path = os.path.join(self.path, item, \"images\", item) + \".png\"\n",
    "        # mask_dir_path = os.path.join(self.path, item, \"masks/\")\n",
    "        # all_masks = os.listdir(mask_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd02cb4-e1b7-4486-bf13-b451e0aaf030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
